import os
import logging
import json
from datetime import datetime
from typing import Dict, Any, Optional
from dotenv import load_dotenv
import requests
from api_llm.utils.helpers import truncar_texto

load_dotenv()
os.makedirs("logs", exist_ok=True)

# ============================
# DOCUMENTACI칍N T칄CNICA: ORIGEN DE DATOS Y ARQUITECTURA
# ============================
# 1. FUENTE DE DATOS: Indexaci칩n vectorial de cat치logo Steam en Elasticsearch.
# 2. ARQUITECTURA: Conexi칩n directa a API remota (OpenRouter) para inferencia.
# 3. ROL DEL LING칖ISTA / PROMPT ENGINEER:
#    - Dise침o del "System Persona" para ajustar el registro comunicativo.
#    - Definici칩n de reglas pragm치ticas para diferenciar "Opini칩n" vs "Venta".
#    - Estrategias de mitigaci칩n de alucinaciones (Grounding).
# ============================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/llm_manager.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================
# Configuraci칩n del Entorno (Solo Remoto)
# ============================
API_KEY = os.getenv("OPENROUTER_API_KEY")
LLM_MODEL = os.getenv("OPENROUTER_MODEL", "google/gemini-2.0-flash-lite-001")

# Hiperpar치metros del modelo LLM
# Temperature: Controla la creatividad (0.7 = balanceado).
# Top_P: Filtra respuestas incoherentes.
LLM_TEMPERATURE = float(os.getenv("LLM_TEMPERATURE", "0.7"))
LLM_TOP_P = float(os.getenv("LLM_TOP_P", "0.9"))
LLM_MAX_TOKENS = int(os.getenv("LLM_MAX_TOKENS", "1000"))

# ============================
# Prompt Engineering (Dise침o Ling칲칤stico)
# ============================
SYSTEM_PROMPT = (
    # --- CAPA 1: DEFINICI칍N DE PERSONA Y REGISTRO ---
    "Act칰a como un experto en videojuegos de Steam. "
    "Tu registro ling칲칤stico debe ser: Amigable, entusiasta, coloquial (jerga gamer) pero respetuoso. "
    "FUENTE DE VERDAD: Tienes acceso exclusivo a un fragmento de base de datos inyectado como 'CONTEXTO'.\n\n"

    # --- CAPA 2: REGLAS PRAGM츼TICAS (INTENCI칍N) ---
    "Tu comportamiento ling칲칤stico se adapta a la intenci칩n del usuario:\n"
    "游꿢 **INTENCI칍N: OPINI칍N (Evaluativa)**\n"
    "   - Estructura: Cr칤tica cualitativa + Menci칩n narrativa de precio.\n"
    "   - Foco: Jugabilidad, historia, mec치nicas.\n"
    "   - Restricci칩n: No listes precios sin contexto narrativo.\n\n"
    
    "游꿢 **INTENCI칍N: B칔SQUEDA/RECOMENDACI칍N (Transaccional)**\n"
    "   - Estructura: Lista Markdown estructurada.\n"
    "   - Foco: Relaci칩n calidad/precio y similitud conceptual.\n\n"

    # --- CAPA 3: RESTRICCIONES SEM츼NTICAS Y GROUNDING ---
    "游 **Reglas de Procesamiento de Informaci칩n:**\n"
    "1. **Principio de Veracidad (Grounding):** Solo puedes ofrecer productos presentes en el CONTEXTO recuperado. "
    "Si el juego no est치 en el contexto, explicita la falta de informaci칩n.\n"
    "2. **Integraci칩n de Conocimiento:** Usa los DATOS del contexto para informaci칩n objetiva (Precios) "
    "y tu ENTRENAMIENTO base para informaci칩n subjetiva (Descripci칩n de diversi칩n).\n\n"

    "游뚿 REGLA SUPREMA: El 'CONTEXTO' es tu 칰nica fuente de datos transaccionales. No inventes precios."
)

# ============================
# Monitor de tokens
# ============================
class TokenMonitor:
    """Registra el uso de tokens y m칠tricas de relevancia"""
    
    def __init__(self):
        self.log_file = "logs/tokens_usage.json"
        os.makedirs("logs", exist_ok=True)
        if not os.path.exists(self.log_file):
            with open(self.log_file, 'w') as f:
                json.dump([], f)
    
    def registrar_uso(self, entrada_tokens: int, salida_tokens: int, modelo: str, pregunta: str, respuesta: str, elastic_score: float = 0.0):
        registro = {
            "timestamp": datetime.now().isoformat(),
            "modelo": modelo,
            "tokens_entrada": entrada_tokens,
            "tokens_salida": salida_tokens,
            "tokens_totales": entrada_tokens + salida_tokens,
            "elastic_score": elastic_score, # M칠trica de calidad de recuperaci칩n
            "pregunta": pregunta[:100],
            "respuesta": respuesta[:500] 
        }
        try:
            with open(self.log_file, 'r') as f:
                datos = json.load(f)
            datos.append(registro)
            with open(self.log_file, 'w') as f:
                json.dump(datos, f, indent=2)
            logger.info(f"Tokens: In={entrada_tokens}/Out={salida_tokens} | Score Elastic: {elastic_score:.4f}")
        except Exception as e:
            logger.error(f"Error registrando tokens: {str(e)}")

# ============================
# Gestor LLM (Solo OpenRouter)
# ============================
class LLMManager:
    """
    Gestor centralizado para la generaci칩n de lenguaje natural.
    Conexi칩n simplificada 칰nicamente a OpenRouter (Gemini).
    
    Limitaciones T칠cnicas (NLP):
    - Alucinaciones: Se mitigan restringiendo la respuesta al contexto inyectado.
    - Dependencia Externa: Se utiliza OpenRouter como proveedor de inferencia.
    """
    
    def __init__(self):
        self.token_monitor = TokenMonitor()
        self.generation_config = {
            "temperature": LLM_TEMPERATURE,
            "top_p": LLM_TOP_P,
            "max_tokens": LLM_MAX_TOKENS
        }
        logger.info(f"LLM Manager Remoto Inicializado | Config: {self.generation_config}")
    
    def obtener_respuesta(self, pregunta: str, contexto: str, elastic_score: float = 0.0) -> Dict[str, Any]:
        """Env칤a la consulta directamente a OpenRouter"""
        
        # Inyecci칩n de contexto RAG
        prompt_usuario = f"DATOS DE CONTEXTO (Corpus):\n{truncar_texto(contexto)}\n\nINPUT USUARIO:\n{pregunta}"
        
        url = "https://openrouter.ai/api/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {API_KEY}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": LLM_MODEL,
            "messages": [
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt_usuario}
            ],
            # Hiperpar치metros din치micos
            "temperature": self.generation_config["temperature"],
            "top_p": self.generation_config["top_p"],
            "max_tokens": self.generation_config["max_tokens"]
        }
        
        try:
            logger.info(f"Enviando consulta a OpenRouter: {LLM_MODEL}")
            response = requests.post(url, headers=headers, json=payload, timeout=60)
            response.raise_for_status()
            data = response.json()
            
            # Validaci칩n simple de respuesta vac칤a
            if not data.get("choices"):
                raise ValueError("La API remota devolvi칩 una respuesta vac칤a.")

            respuesta = data["choices"][0]["message"]["content"]
            
            # M칠tricas
            usage = data.get("usage", {})
            tokens_entrada = usage.get("prompt_tokens", 0)
            tokens_salida = usage.get("completion_tokens", 0)
            
            self.token_monitor.registrar_uso(tokens_entrada, tokens_salida, LLM_MODEL, pregunta, respuesta, elastic_score)
            
            return {
                "respuesta": respuesta.strip(),
                "tokens_entrada": tokens_entrada,
                "tokens_salida": tokens_salida,
                "elastic_score": elastic_score,
                "modelo": LLM_MODEL,
                "error": None
            }
            
        except Exception as e:
            logger.error(f"Error cr칤tico en LLM: {str(e)}")
            return self._generar_respuesta_error(str(e))
    
    def _generar_respuesta_error(self, error_msg: str) -> Dict[str, Any]:
        return {
            "respuesta": f"Lo siento, tengo un problema t칠cnico de conexi칩n y no puedo responderte ahora mismo. (Error: {error_msg})",
            "tokens_entrada": 0,
            "tokens_salida": 0,
            "modelo": None,
            "error": error_msg
        }

def obtener_respuesta_llm(pregunta: str, contexto: str, elastic_score: float = 0.0) -> str:
    manager = LLMManager()
    resultado = manager.obtener_respuesta(pregunta, contexto, elastic_score)
    return resultado["respuesta"]