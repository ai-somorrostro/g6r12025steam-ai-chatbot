**ğŸ“˜ DocumentaciÃ³n detallada - Sistema de Benchmarking de Modelos (LLM y Embeddings)**

---

### ğŸ” Objetivo General del Benchmark

El objetivo principal ha sido implementar un sistema de **benchmarking automÃ¡tico** que permita evaluar y comparar:

* âœ… **Modelos LLM** usados para tareas de generaciÃ³n de texto en la API.
* âœ… **Modelos de embeddings** usados para tareas de recuperaciÃ³n semÃ¡ntica (RAG).

Este sistema permite tomar decisiones informadas sobre quÃ© modelo utilizar segÃºn diferentes mÃ©tricas de rendimiento, coste, precisiÃ³n y consumo de recursos.

---

### ğŸ“‚ Estructura y UbicaciÃ³n de Archivos

```
API-Reto-1/
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ benchmark_llm.log             # Log de resultados del benchmark de modelos LLM
â”‚   â””â”€â”€ benchmark_embeddings.log      # Log de resultados del benchmark de modelos de embeddings
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_benchmark_llm.py         # Script para benchmark de modelos LLM (tiempos, tokens, coste)
â”‚   â”œâ”€â”€ test_benchmark_embeddings.py  # Script para benchmark de embeddings (tiempos, RAM, similitud)
â”‚   â”œâ”€â”€ graficar_resultados_embeddings.py  # Script para generar grÃ¡ficas desde el log de embeddings
â”‚   â””â”€â”€ plots/
â”‚       â””â”€â”€ benchmark_embeddings.pdf  # Documento generado con grÃ¡ficas visuales comparativas
```

---

### ğŸ“… Proceso realizado paso a paso

#### 1. **Benchmark de LLMs** (`test_benchmark_llm.py`)

Se evaluaron modelos como `gemini`, `mistral`, `openrouter`, etc. Se registran:

* Tiempo de respuesta
* Tokens generados
* Coste estimado por peticiÃ³n
* Tokens por segundo (rendimiento)

**Resultado**: Un log detallado con todas las pruebas guardado en `benchmark_llm.log`

#### 2. **Benchmark de modelos de embeddings** (`test_benchmark_embeddings.py`)

Modelos evaluados:

* `sentence-transformers/all-MiniLM-L6-v2`
* `intfloat/multilingual-e5-base`
* `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
* `sentence-transformers/paraphrase-multilingual-mpnet-base-v2`

**MÃ©tricas analizadas:**

* Tiempo de carga del modelo
* Tiempo de inferencia
* Uso de RAM
* TamaÃ±o en disco
* PrecisiÃ³n semÃ¡ntica (cosine similarity)

**Resultado:** Log `benchmark_embeddings.log` con toda la informaciÃ³n estructurada.

#### 3. **GeneraciÃ³n de grÃ¡ficas** (`graficar_resultados_embeddings.py`)

Se genera un PDF con las siguientes grÃ¡ficas:

| Tipo de GrÃ¡fico             | MÃ©trica Representada                         |
| --------------------------- | -------------------------------------------- |
| Barras horizontales         | Tiempo de carga                              |
| Barras horizontales         | Tiempo de inferencia                         |
| Barras horizontales         | RAM utilizada                                |
| Barras horizontales         | TamaÃ±o en disco                              |
| Barras horizontales         | PrecisiÃ³n semÃ¡ntica (cosine)                 |
| **Mapa de calor (heatmap)** | Comparativa global normalizada entre modelos |

---

### ğŸ“Š ExplicaciÃ³n del Heatmap Final (Sustituto del Radar Chart)

El Ãºltimo grÃ¡fico generado es un **heatmap** que resume todas las mÃ©tricas en una sola visualizaciÃ³n:

* Cada celda muestra el **rendimiento relativo normalizado** de un modelo en una mÃ©trica.
* Valores altos (cercanos a 1) implican **mejor rendimiento**.
* Colores oscuros â†’ mejor resultado. Colores claros â†’ peor.

âœ‰ï¸ Este reemplaza el radar chart, que resultaba poco legible o vacÃ­o si habÃ­a datos faltantes.

---

### ğŸ“¦ EjecuciÃ³n paso a paso del sistema de benchmarking
Esta secciÃ³n describe el orden de ejecuciÃ³n recomendado para evaluar y comparar los modelos LLM y de embeddings implementados en este proyecto. AsegÃºrate de haber instalado todas las dependencias necesarias (ver requirements.txt) antes de comenzar.

ğŸ”¹ Paso 1: Activar el entorno virtual
```
cd API-Reto-1
source venv/bin/activate
```

ğŸ”¹ Paso 2: Benchmark de modelos LLM

    âœ… Script: tests/test_benchmark_llm.py

Este script realiza pruebas de rendimiento sobre diferentes modelos LLM conectados vÃ­a OpenRouter o proveedores externos. EvalÃºa:

* Tiempo de respuesta.

* Tokens usados (prompt, completion y total).

* Tokens por segundo.

* Coste estimado por consulta.

* Calidad bÃ¡sica de respuesta (si se configura).


ğŸ“¦ Resultado:

Se genera el fichero de logs logs/benchmark_llm.log.

â–¶ï¸ EjecuciÃ³n:

```
python tests/test_benchmark_llm.py
```

ğŸ”¹ Paso 3: GeneraciÃ³n de grÃ¡ficas del benchmark LLM

    âœ… Script: tests/graficar_resultados_llm.py

Este script procesa el log generado anteriormente (benchmark_llm.log) y crea un fichero PDF con varias visualizaciones:

* Tiempo de respuesta por modelo.

* Tokens usados (prompt/completion/total).

* Coste por cada consulta.

* Velocidad de respuesta (tokens/segundo).

* Calidad bÃ¡sica si estÃ¡ configurada.

ğŸ“„ Resultado:

* Fichero PDF generado en tests/plots/benchmark_llm.pdf.

â–¶ï¸ EjecuciÃ³n:
```
python tests/graficar_resultados_llm.py
```


ğŸ”¹ Paso 4: Benchmark de modelos de embeddings

    âœ… Script: tests/test_benchmark_embeddings.py

EvalÃºa varios modelos de embeddings locales (e.g., de sentence-transformers o intfloat). Calcula:

* Tiempo de carga.

* Tiempo de inferencia.

* Uso de RAM.

* TamaÃ±o en disco.

* PrecisiÃ³n semÃ¡ntica basada en similitud coseno entre frases.

ğŸ“¦ Resultado:

* Se genera el fichero de logs logs/benchmark_embeddings.log.


â–¶ï¸ EjecuciÃ³n:
```
python tests/test_benchmark_embeddings.py
```


ğŸ”¹ Paso 5: GeneraciÃ³n de grÃ¡ficas del benchmark de embeddings

    âœ… Script: tests/graficar_resultados_embeddings.py

Este script lee el log benchmark_embeddings.log y genera un PDF con los siguientes grÃ¡ficos:

* Tiempo de carga.

* Tiempo de inferencia.

* Memoria RAM utilizada.

* TamaÃ±o en disco.

* PrecisiÃ³n semÃ¡ntica.

* Comparativa global mediante heatmap normalizado (sustituciÃ³n del grÃ¡fico radar por uno mÃ¡s legible y claro).

ğŸ“„ Resultado:

Fichero PDF generado en tests/plots/benchmark_embeddings.pdf.


â–¶ï¸ EjecuciÃ³n:

```
python tests/graficar_resultados_embeddings.py
```

ğŸ“Œ Observaciones importantes

Si al ejecutar algÃºn script falta una librerÃ­a, instÃ¡lala con pip install y aÃ±ade el paquete al requirements.txt.

Todos los logs quedan en logs/, y todas las grÃ¡ficas se guardan como PDF en tests/plots/.

Se recomienda mantener limpios los logs antes de una nueva ejecuciÃ³n para evitar mezclar resultados.

Con estos pasos, cualquier usuario podrÃ¡ reproducir de forma controlada y completa el sistema de benchmarking del proyecto y visualizar los resultados sin complicaciones.


### ğŸš€ ConclusiÃ³n

El sistema de benchmark ha sido **automatizado, visualizado y documentado** con:

* Ejecuciones reproducibles
* Logs estructurados
* Visualizaciones comparativas claras
* DocumentaciÃ³n integrada y exportable

Con esto se garantiza la capacidad de auditar, escalar o sustituir modelos en base a datos objetivos y comparables.
