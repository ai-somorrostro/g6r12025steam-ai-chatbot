# JustificaciÃ³n detallada de las mejoras implementadas: Benchmarking de modelos LLM y Embeddings

## IntroducciÃ³n

Como parte de las mejoras solicitadas en el reto del mÃ³dulo PIA, se nos propuso implementar un sistema de **benchmarking** para evaluar y comparar distintos modelos LLM (Large Language Models) y modelos de embeddings. El objetivo principal era **tomar decisiones fundamentadas** sobre los modelos a utilizar en la API del proyecto, atendiendo a criterios como rendimiento, coste, velocidad y precisiÃ³n.

A continuaciÃ³n se justifica detalladamente **quÃ© se ha hecho**, **por quÃ© se ha hecho**, y **cÃ³mo se ha implementado** cada una de las dos partes del benchmark: **LLMs** y **Embeddings**, apoyÃ¡ndose en los archivos desarrollados.

---

## Benchmarking de Modelos LLM (Large Language Models)

### ðŸ” QuÃ© se pedÃ­a:

* Evaluar distintos modelos LLM.
* Compararlos en tÃ©rminos de:

  * Tiempo de respuesta (latencia media, p95, p99)
  * Velocidad de generaciÃ³n (tokens/s)
  * Tokens usados (entrada/salida)
  * Coste estimado por solicitud (tokens * precio modelo)
  * Calidad de respuesta (opcional, se registra la respuesta)

### ðŸ“„ Archivos desarrollados:

#### 1. `tests/test_benchmark_llm.py`

* Ejecuta las consultas definidas contra cada modelo.
* Usa `LLMManager` para simular peticiones reales.
* Recupera contexto vÃ­a Elasticsearch (como se harÃ­a en la API real).
* Mide tiempos, tokens generados, coste y velocidad.
* Guarda todo en el fichero de log: `logs/benchmark_llm.log`.

#### 2. `tests/graficar_resultados_llm.py`

* Lee el log generado y extrae los datos por modelo.
* Genera un PDF (`graficas_llm.pdf`) con 4 grÃ¡ficos:

  1. Boxplot de latencia por modelo.
  2. Tokens por segundo (velocidad).
  3. Coste total estimado.
  4. Tokens generados promedio.

### ðŸ§µ Modelos evaluados:

* `google/gemini-2.0-flash-lite-001`
* `mistralai/mixtral-8x7b`
* `meta-llama/llama-3-8b`

### âœ… JustificaciÃ³n:

* **Modelos seleccionados**: los 3 modelos evaluados se eligieron por ser los mÃ¡s accesibles desde la API externa utilizada (`OpenRouter`) y representar distintas gamas (ligero, medio, potente).
* **Modelo usado finalmente en la API**: `google/gemini-2.0-flash-lite-001`.

  * JustificaciÃ³n: fue el modelo con mejor relaciÃ³n coste-velocidad-calidad para el tipo de consultas que hacemos (preguntas sobre videojuegos con contexto semÃ¡ntico).

### ðŸ“Š Resultados de las grÃ¡ficas:

* `Gemini` tiene el menor tiempo de respuesta promedio.
* `LLaMA 3` genera mÃ¡s tokens por segundo que `Mixtral`.
* `Mixtral` presenta mayor coste total acumulado.

---

## Benchmarking de Modelos de Embeddings

### ðŸ” QuÃ© se pedÃ­a:

* Comparar distintos modelos de embeddings considerando:

  * Tiempo de carga
  * Tiempo de inferencia sobre 100 textos
  * Uso de RAM
  * TamaÃ±o en disco del modelo
  * PrecisiÃ³n semÃ¡ntica (similitud cosine)

### ðŸ“„ Archivos desarrollados:

#### 1. `tests/test_benchmark_embeddings.py`

* EvalÃºa 4 modelos de embeddings descargables de Hugging Face.
* Mide:

  * Tiempo de carga del modelo.
  * Tiempo de inferencia sobre un batch de 100 textos.
  * RAM usada en inferencia.
  * PrecisiÃ³n media comparando pares de textos similares con `cosine_similarity`.
  * TamaÃ±o del modelo en disco.
* Guarda todo en `logs/benchmark_embeddings.log`.

#### 2. `tests/graficar_resultados_embeddings.py`

* Lee el log anterior.
* Genera el PDF `benchmark_embeddings.pdf` con 6 grÃ¡ficos:

  1. Barras de tiempo de carga.
  2. Barras de inferencia.
  3. Barras de RAM usada.
  4. Barras de tamaÃ±o.
  5. Barras de precisiÃ³n.
  6. Heatmap comparativo global normalizado.

### ðŸ§µ Modelos evaluados:

* `sentence-transformers/all-MiniLM-L6-v2`
* `intfloat/multilingual-e5-base`
* `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2`
* `sentence-transformers/paraphrase-multilingual-mpnet-base-v2` âœ… (modelo usado en la API)

### âœ… JustificaciÃ³n:

* Se evaluaron modelos multilingÃ¼es y ligeros con alto rendimiento en tareas semÃ¡nticas.
* El modelo elegido para la API es `paraphrase-multilingual-mpnet-base-v2` porque:

  * Ofrece **alta precisiÃ³n semÃ¡ntica** (cosine similarity > 0.7).
  * Aceptable tiempo de inferencia y carga.
  * Buen equilibrio entre **calidad y rendimiento**.

### ðŸ“Š Resultados de las grÃ¡ficas:

* `MiniLM-L6` es el mÃ¡s rÃ¡pido y ligero.
* `MPNET` (el que usamos) no es el mÃ¡s rÃ¡pido, pero **es el mÃ¡s equilibrado**.
* `E5-base` destaca en precisiÃ³n pero tarda mÃ¡s en inferencia.
* Heatmap permite comparar todos los factores en una sola visualizaciÃ³n.

---

## Estructura del repositorio relevante

```
API-Reto-1/
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ benchmark_llm.log
â”‚   â”œâ”€â”€ benchmark_embeddings.log
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_benchmark_llm.py
â”‚   â”œâ”€â”€ graficar_resultados_llm.py
â”‚   â”œâ”€â”€ test_benchmark_embeddings.py
â”‚   â”œâ”€â”€ graficar_resultados_embeddings.py
â”‚   â””â”€â”€ plots/
â”‚       â”œâ”€â”€ benchmark_embeddings.pdf
â”‚       â”œâ”€â”€ graficas_llm.pdf
â”œâ”€â”€ benchmark_doc.md âœ… Documento principal
```

---

## LibrerÃ­as adicionales usadas

* `matplotlib` â†’ grÃ¡ficos de barras, boxplot y PDFs.
* `seaborn` â†’ heatmap comparativo.
* `psutil` â†’ medir uso de RAM.
* `sentence-transformers`, `sklearn` â†’ embeddings y similitud cosine.
* `numpy` â†’ estadÃ­sticas (medias, percentiles).

---

## CÃ³mo se ejecuta cada parte y en quÃ© orden

### Paso 1: Ejecutar benchmark LLM

```bash
python tests/test_benchmark_llm.py
```

* Genera `logs/benchmark_llm.log`

### Paso 2: Generar PDF de resultados del LLM

```bash
python tests/graficar_resultados_llm.py
```

* Genera `tests/plots/graficas_llm.pdf`

### Paso 3: Ejecutar benchmark de embeddings

```bash
python tests/test_benchmark_embeddings.py
```

* Genera `logs/benchmark_embeddings.log`

### Paso 4: Generar PDF de resultados de embeddings

```bash
python tests/graficar_resultados_embeddings.py
```

* Genera `tests/plots/benchmark_embeddings.pdf`

---

## Consideraciones finales

* Se han cumplido todos los objetivos planteados de evaluaciÃ³n de modelos.
* Se han seleccionado **modelos reales utilizados en la API** (no se ha hecho de forma aislada).
* Se han generado **logs trazables y visualizaciones exportables** para la presentaciÃ³n.
* La documentaciÃ³n final estÃ¡ centralizada en el archivo `benchmark_doc.md`.

---

## Posibles mejoras futuras

* EvaluaciÃ³n automÃ¡tica de la calidad de respuestas LLM ("LLM-as-a-judge").
* Benchmark de consumo real en GPUs.
* ComparaciÃ³n con embeddings propios entrenados.

---

## ConclusiÃ³n

Este trabajo de benchmarking ha permitido tomar decisiones tÃ©cnicas justificadas sobre quÃ© modelos usar en producciÃ³n, optimizando el equilibrio entre **calidad, coste y eficiencia**. Se han documentado, visualizado y versionado todos los pasos.
