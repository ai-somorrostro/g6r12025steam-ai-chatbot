# # âœ… tests/test_benchmark_embeddings.py
# # Benchmark para evaluar modelos de embeddings

# import time
# import os
# import psutil
# import sys
# import importlib
# import logging
# from pathlib import Path
# from sentence_transformers import SentenceTransformer

# # ğŸ“ Asegura ruta raÃ­z del proyecto
# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# # ğŸ“ Logging
# log_dir = os.path.join(os.path.dirname(__file__), "..", "logs")
# os.makedirs(log_dir, exist_ok=True)
# log_file = os.path.join(log_dir, "benchmark_embeddings.log")

# logger = logging.getLogger("benchmark_logger_embeddings")
# logger.setLevel(logging.INFO)
# formatter = logging.Formatter('%(asctime)s - %(message)s')
# file_handler = logging.FileHandler(log_file, mode='w')
# file_handler.setFormatter(formatter)
# logger.addHandler(file_handler)

# # ğŸ§ª Modelos a comparar
# modelos = [
#     "sentence-transformers/all-MiniLM-L6-v2",
#     "intfloat/multilingual-e5-base",
#     "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
# ]

# # ğŸ“‹ Batch de textos para evaluar
# textos = [
#     "Â¿QuÃ© juegos de simulaciÃ³n hay?",
#     "Quiero juegos con buena historia.",
#     "Busco algo de terror psicolÃ³gico.",
#     "Â¿Tienes juegos con multijugador local?",
#     "Â¿CuÃ¡les son los mÃ¡s baratos?",
# ] * 20  # â• Multiplicamos para simular batch de 100

# def medir_rendimiento_embedding(model_name: str):
#     logger.info(f"ğŸ” Evaluando modelo: {model_name}")

#     # â±ï¸ Carga del modelo
#     start = time.time()
#     model = SentenceTransformer(model_name)
#     tiempo_carga = time.time() - start

#     # ğŸ§  TamaÃ±o en disco
#     model_path = Path(model.cache_folder).joinpath(model_name.replace("/", "_"))
#     if model_path.exists():
#         peso_mb = sum(f.stat().st_size for f in model_path.glob("**/*") if f.is_file()) / (1024 * 1024)
#     else:
#         peso_mb = 0

#     # ğŸ’¾ Memoria usada antes de la inferencia
#     mem_inicio = psutil.Process(os.getpid()).memory_info().rss

#     # â±ï¸ Inferencia
#     start = time.time()
#     _ = model.encode(textos, batch_size=32, show_progress_bar=False)
#     tiempo_inferencia = time.time() - start

#     # ğŸ’¾ Memoria despuÃ©s
#     mem_final = psutil.Process(os.getpid()).memory_info().rss
#     mem_usada_mb = (mem_final - mem_inicio) / (1024 * 1024)

#     # ğŸ“¤ Log result
#     logger.info(f"ğŸ§  Tiempo de carga: {tiempo_carga:.2f}s")
#     logger.info(f"âš¡ Tiempo de inferencia (100 textos): {tiempo_inferencia:.2f}s")
#     logger.info(f"ğŸ“¦ TamaÃ±o en disco: {peso_mb:.2f} MB")
#     logger.info(f"ğŸ’¾ RAM utilizada en inferencia: {mem_usada_mb:.2f} MB")
#     logger.info("=" * 70)

# def run_benchmark_embeddings():
#     for modelo in modelos:
#         medir_rendimiento_embedding(modelo)

# if __name__ == "__main__":
#     run_benchmark_embeddings()






# âœ… tests/test_benchmark_embeddings.py
# Benchmark para evaluar modelos de embeddings

import time
import os
import psutil
import sys
import logging
import numpy as np
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# ğŸ“ Asegura ruta raÃ­z del proyecto
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# ğŸ“ Logging
log_dir = os.path.join(os.path.dirname(__file__), "..", "logs")
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, "benchmark_embeddings.log")

logger = logging.getLogger("benchmark_logger_embeddings")
logger.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(message)s')
file_handler = logging.FileHandler(log_file, mode='w')
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# ğŸ§ª Modelos a comparar
modelos = [
    "sentence-transformers/all-MiniLM-L6-v2",
    "intfloat/multilingual-e5-base",
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"  # âœ… Tu modelo real
]

# ğŸ“‹ Textos para similitud semÃ¡ntica (pares)
pares_textos = [
    ("Juego de terror psicolÃ³gico", "Juego con ambiente de miedo"),
    ("Simulador de granja", "SimulaciÃ³n de vida rural"),
    ("Disparos en primera persona", "FPS con armas de fuego"),
    ("Multijugador local", "Jugar en la misma pantalla"),
    ("Estrategia medieval", "Juego de castillos y reinos"),
]

# ğŸ“‹ Textos para benchmark general
textos = [t[0] for t in pares_textos] * 20  # â• Simulamos 100 textos

def medir_rendimiento_embedding(model_name: str):
    logger.info(f"ğŸ” Evaluando modelo: {model_name}")

    # â±ï¸ Carga del modelo
    start = time.time()
    model = SentenceTransformer(model_name)
    tiempo_carga = time.time() - start

    # ğŸ§  TamaÃ±o en disco
    try:
        model_path = Path(model._first_module().model_dir)
        peso_mb = sum(f.stat().st_size for f in model_path.glob("**/*") if f.is_file()) / (1024 * 1024)
    except Exception:
        peso_mb = 0

    # ğŸ’¾ Memoria usada antes de la inferencia
    mem_inicio = psutil.Process(os.getpid()).memory_info().rss

    # â±ï¸ Inferencia
    start = time.time()
    _ = model.encode(textos, batch_size=32, show_progress_bar=False)
    tiempo_inferencia = time.time() - start

    # ğŸ’¾ Memoria despuÃ©s
    mem_final = psutil.Process(os.getpid()).memory_info().rss
    mem_usada_mb = (mem_final - mem_inicio) / (1024 * 1024)

    # ğŸ¯ EvaluaciÃ³n de precisiÃ³n de similitud semÃ¡ntica
    precisiones = []
    for t1, t2 in pares_textos:
        emb1 = model.encode(t1)
        emb2 = model.encode(t2)
        sim = cosine_similarity([emb1], [emb2])[0][0]
        precisiones.append(sim)
    precision_media = np.mean(precisiones)

    # ğŸ“¤ Log result
    logger.info(f"ğŸ§  Tiempo de carga: {tiempo_carga:.2f}s")
    logger.info(f"âš¡ Tiempo de inferencia (100 textos): {tiempo_inferencia:.2f}s")
    logger.info(f"ğŸ“¦ TamaÃ±o en disco: {peso_mb:.2f} MB")
    logger.info(f"ğŸ’¾ RAM utilizada en inferencia: {mem_usada_mb:.2f} MB")
    logger.info(f"ğŸ¯ PrecisiÃ³n media (cosine): {precision_media:.4f}")
    logger.info("=" * 70)

def run_benchmark_embeddings():
    for modelo in modelos:
        medir_rendimiento_embedding(modelo)

if __name__ == "__main__":
    run_benchmark_embeddings()
